{"cells":[{"cell_type":"markdown","id":"2e36a3c0-2758-49ea-9207-2ab40cde8d5a","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/images/IDSN-logo.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"45683a01","metadata":{},"outputs":[],"source":["# Getting Started with PySpark and Pandas\n"]},{"cell_type":"markdown","id":"6b9a57ca","metadata":{},"outputs":[],"source":["Estimated time needed: **60** minutes\n"]},{"cell_type":"markdown","id":"24043296","metadata":{},"outputs":[],"source":["PySpark is the Python API for Apache Spark, a distributed computing system designed for handling large-scale data processing. Its ability to perform computations across multiple nodes makes it an ideal choice for Big Data scenarios, where data sets can reach terabytes or even petabytes in size. PySpark allows data scientists and analysts to harness the power of distributed computing, scaling up their workflows and processing capabilities significantly.\n"]},{"cell_type":"markdown","id":"7349c3d9","metadata":{},"outputs":[],"source":["In contrast, Pandas is a powerful library tailored for data manipulation and analysis in Python, primarily used for handling structured data. It provides rich functionality for data cleaning, transformation, aggregation, and visualization, making it particularly suited for smaller data sets typically fitting into memory. Pandas excels at operations requiring quick turnaround times for data analysis and exploration, facilitating tasks such as data wrangling and exploratory data analysis.\n"]},{"cell_type":"markdown","id":"29e424b8","metadata":{},"outputs":[],"source":["To illustrate the strengths of both PySpark and Pandas, we will utilize a sample COVID-19 data set containing daily cases, deaths, and vaccinations across various continents.\n"]},{"cell_type":"markdown","id":"5f0b56f5","metadata":{},"outputs":[],"source":["## Objectives\n"]},{"cell_type":"markdown","id":"033fa373","metadata":{},"outputs":[],"source":["- Understand PySpark and Pandas: Explain the core functionalities and use cases of PySpark for big data processing and Pandas for data manipulation.\n"]},{"cell_type":"markdown","id":"2fa9ccdb","metadata":{},"outputs":[],"source":["- Set up the environment: Install and configure PySpark and Pandas to work together in a Python environment.\n"]},{"cell_type":"markdown","id":"d148f1f4","metadata":{},"outputs":[],"source":["- Load and explore data: Import data into Pandas and PySpark DataFrames and perform basic data exploration.\n"]},{"cell_type":"markdown","id":"a9d68df7","metadata":{},"outputs":[],"source":["- Convert between DataFrames: Convert a Pandas DataFrame to a Spark DataFrame for distributed processing.\n"]},{"cell_type":"markdown","id":"488ef90c","metadata":{},"outputs":[],"source":["- Perform data manipulation: Create new columns, filter data, and perform aggregations using PySpark.\n"]},{"cell_type":"markdown","id":"460420c5","metadata":{},"outputs":[],"source":["- Utilize SQL queries: Use Spark SQL for querying data and leveraging user-defined functions (UDFs).\n"]},{"cell_type":"markdown","id":"3a640c77","metadata":{},"outputs":[],"source":["## 1. PySpark overview\n"]},{"cell_type":"markdown","id":"b322f2c4","metadata":{},"outputs":[],"source":["### Overview\n"]},{"cell_type":"markdown","id":"54de6c53","metadata":{},"outputs":[],"source":["PySpark is the Python API for Apache Spark, designed for large-scale data processing and analysis. It offers tools for working with RDDs and DataFrames, enabling efficient, fault-tolerant distributed computing.\n"]},{"cell_type":"markdown","id":"3d27e4e6","metadata":{},"outputs":[],"source":["### Key features\n"]},{"cell_type":"markdown","id":"c9b2fc44","metadata":{},"outputs":[],"source":["- **Distributed computing:** Handles data across multiple nodes in a cluster.\n","- **High performance:** Outperforms traditional frameworks in speed.\n","- **Big data handling:** Manages data sets larger than a single machine's memory.\n","- **Python integration:** Compatible with Python libraries like Pandas and NumPy.\n"]},{"cell_type":"markdown","id":"d320446e","metadata":{},"outputs":[],"source":["### Use cases\n"]},{"cell_type":"markdown","id":"24119fbb","metadata":{},"outputs":[],"source":["- **Large-scale data processing:** Ideal for processing large volumes of data that exceed the capacity of a single machine.\n","- **Data analysis:** Useful for complex data manipulations and analysis using distributed computing.\n"]},{"cell_type":"markdown","id":"a5e14c42","metadata":{},"outputs":[],"source":["### Strengths\n"]},{"cell_type":"markdown","id":"96274411","metadata":{},"outputs":[],"source":["- High-speed data processing.\n","- Fault-tolerant and scalable.\n"]},{"cell_type":"markdown","id":"af86b810","metadata":{},"outputs":[],"source":["### Limitations\n"]},{"cell_type":"markdown","id":"c5448258","metadata":{},"outputs":[],"source":["- Complex setup and configuration.\n","- Steeper learning curve compared to some data processing tools.\n"]},{"cell_type":"markdown","id":"c935863d","metadata":{},"outputs":[],"source":["## 2. Understanding Pandas\n"]},{"cell_type":"markdown","id":"80dcf807","metadata":{},"outputs":[],"source":["### Overview\n"]},{"cell_type":"markdown","id":"51f45249","metadata":{},"outputs":[],"source":["Pandas is a Python library designed for data manipulation and analysis. It provides two primary data structures: Series and DataFrame, which facilitate handling and organizing structured data.\n"]},{"cell_type":"markdown","id":"5b1fc806","metadata":{},"outputs":[],"source":["### Key features\n"]},{"cell_type":"markdown","id":"727c7c4e","metadata":{},"outputs":[],"source":["- **Data structures:** Series for one-dimensional data and DataFrame for two-dimensional data.\n","- **Data I/O:** Reads and writes data in various formats such as CSV, Excel, and SQL.\n","- **Data cleaning:** Functions for handling missing or duplicate data.\n","- **Data analysis:** Includes statistical functions for detailed data analysis.\n"]},{"cell_type":"markdown","id":"ea6356ef","metadata":{},"outputs":[],"source":["### Use cases\n"]},{"cell_type":"markdown","id":"a3c32156","metadata":{},"outputs":[],"source":["- **Data manipulation:** Efficient handling of structured data.\n","- **Data analysis:** Comprehensive analysis and transformation of data sets.\n"]},{"cell_type":"markdown","id":"1e4a0ae0","metadata":{},"outputs":[],"source":["### Strengths\n"]},{"cell_type":"markdown","id":"e393764e","metadata":{},"outputs":[],"source":["- User-friendly API for data manipulation.\n","- Extensive support for various data formats.\n"]},{"cell_type":"markdown","id":"88e01276","metadata":{},"outputs":[],"source":["### Limitations\n"]},{"cell_type":"markdown","id":"be5751ca","metadata":{},"outputs":[],"source":["- Limited scalability for extremely large datasets compared to distributed frameworks.\n"]},{"cell_type":"markdown","id":"e94abbec","metadata":{},"outputs":[],"source":["## 3. Setting up the environment\n"]},{"cell_type":"markdown","id":"586b112a","metadata":{},"outputs":[],"source":["### Installation\n"]},{"cell_type":"markdown","id":"6f90637f","metadata":{},"outputs":[],"source":["- First, let's install the necessary libraries if they are not already installed.\n"]},{"cell_type":"code","id":"b46e53fd","metadata":{},"outputs":[],"source":["!pip install pyspark"]},{"cell_type":"code","id":"c6e9b756","metadata":{},"outputs":[],"source":["!pip install findspark"]},{"cell_type":"code","id":"1fd31ec1","metadata":{},"outputs":[],"source":["!pip install pandas"]},{"cell_type":"markdown","id":"637700c3","metadata":{},"outputs":[],"source":["## 4. Initializing a Spark session\n"]},{"cell_type":"markdown","id":"4fe45539","metadata":{},"outputs":[],"source":["A Spark session is crucial for working with PySpark. It enables DataFrame creation, data loading, and various operations.\n"]},{"cell_type":"markdown","id":"7e3f2e74","metadata":{},"outputs":[],"source":["### Importing libraries\n"]},{"cell_type":"markdown","id":"803037b1","metadata":{},"outputs":[],"source":["- `findspark` is used to locate the Spark installation.\n","- `pandas` is imported for data manipulation.\n"]},{"cell_type":"markdown","id":"899663ca","metadata":{},"outputs":[],"source":["### Creating a Spark session\n"]},{"cell_type":"markdown","id":"719b7636-87d3-4a8c-a0b1-6be5f72c02fd","metadata":{},"outputs":[],"source":["- `SparkSession.builder.appName(\"COVID-19 Data Analysis\").getOrCreate()` initializes a Spark session with the specified application name.\n"]},{"cell_type":"markdown","id":"e8390c72","metadata":{},"outputs":[],"source":["### Checking Spark session\n"]},{"cell_type":"markdown","id":"45736a03-145b-4117-937a-77bbfe5e9bb8","metadata":{},"outputs":[],"source":["- The code checks if the Spark session is active and prints an appropriate message.\n"]},{"cell_type":"code","id":"8f8f4a9b","metadata":{},"outputs":[],"source":["import findspark  # This helps us find and use Apache Spark\nfindspark.init()  # Initialize findspark to locate Spark\nfrom pyspark.sql import SparkSession  \nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DateType\nimport pandas as pd  \n# Initialize a Spark Session\nspark = SparkSession \\\n    .builder \\\n    .appName(\"COVID-19 Data Analysis\") \\\n    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n    .getOrCreate()\n\n# Check if the Spark Session is active\nif 'spark' in locals() and isinstance(spark, SparkSession):\n    print(\"SparkSession is active and ready to use.\")\nelse:\n    print(\"SparkSession is not active. Please create a SparkSession.\")"]},{"cell_type":"markdown","id":"1e9cb184","metadata":{},"outputs":[],"source":["## 5. Importing data into Pandas from various sources\n"]},{"cell_type":"markdown","id":"2642eaa7","metadata":{},"outputs":[],"source":["Let's read the COVID-19 data from the provided URL.\n"]},{"cell_type":"code","id":"28aaa452","metadata":{},"outputs":[],"source":["# Read the COVID-19 data from the provided URL\nvaccination_data = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/KpHDlIzdtR63BdTofl1mOg/owid-covid-latest.csv')"]},{"cell_type":"markdown","id":"6f671a11","metadata":{},"outputs":[],"source":["## 6. Displaying the first five records\n"]},{"cell_type":"markdown","id":"b05e50f5-5e31-4956-b9b7-256e97893868","metadata":{},"outputs":[],"source":["### To retrieve and print the first five records\n"]},{"cell_type":"markdown","id":"2dd7f92e-961c-40f2-9b3c-1a389bf06367","metadata":{},"outputs":[],"source":["- `vaccination_data.head()` retrieves the first five rows of the DataFrame vaccination_data.This gives us a quick look at the data contained within the data set.\n","- The `print` function is used to display a message indicating what is being shown, followed by the actual data.\n"]},{"cell_type":"markdown","id":"adf06498-3710-4a1d-a947-409cee745bd6","metadata":{},"outputs":[],"source":["### Selecting specific columns:\n"]},{"cell_type":"markdown","id":"a889c593-465f-4405-9165-bef58bb2d574","metadata":{},"outputs":[],"source":["- Let\\'s define a list called `columns_to_display`, which contains the names of the columns as : `['continent', 'total_cases', 'total_deaths', 'total_vaccinations', 'population']`.\n","- By using `vaccination_data[columns_to_display].head()`, let\\'s filter the DataFrame to only show the specified columns and again display the first five records of this subset.\n","- The continent column is explicitly converted to string, while the numeric columns (total cases, total deaths, total vaccinations, population) are filled with zeros for NaN values and then converted to int64 (which is compatible with LongType in Spark).\n","- The use of fillna(0) ensures that NaN values do not cause type issues during the Spark DataFrame creation.\n"]},{"cell_type":"code","id":"d1267201","metadata":{},"outputs":[],"source":["print(\"Displaying the first 5 records of the vaccination data:\")\ncolumns_to_display = ['continent', 'total_cases', 'total_deaths', 'total_vaccinations', 'population']\n# Show the first 5 records\nprint(vaccination_data[columns_to_display].head())"]},{"cell_type":"markdown","id":"b42d9256","metadata":{},"outputs":[],"source":["## 7. Converting the Pandas DataFrame to a Spark DataFrame\n"]},{"cell_type":"markdown","id":"e8c51087","metadata":{},"outputs":[],"source":["Let\\'s convert the Pandas DataFrame, which contains our COVID-19 vaccination data, into a Spark DataFrame. This conversion is crucial as it allows us to utilize Spark\\'s distributed computing capabilities, enabling us to handle larger datasets and perform operations in a more efficient manner.\n"]},{"cell_type":"markdown","id":"75c305eb-4aff-43f5-91c9-e827f2e6a1c4","metadata":{},"outputs":[],"source":["### Defining the schema:\n"]},{"cell_type":"markdown","id":"a6b8c3ac-9e8a-40fa-a82c-d5eaf1008f8b","metadata":{},"outputs":[],"source":["- **StructType**: \n","  - A class that defines a structure for a DataFrame.\n","\n","- **StructField**: \n","  - Represents a single field in the schema.\n","  - **Parameters**:\n","    1. **Field name**: The name of the field.\n","    2. **Data type**: The type of data for the field.\n","    3. **Nullable**: A boolean indicating whether null values are allowed.\n","\n","- **Data types**:\n","  - **StringType()**: Used for text fields.\n","  - **LongType()**: Used for numerical fields.\n"]},{"cell_type":"markdown","id":"95e48806-3bfb-49ff-8dbf-ce2f4389ac67","metadata":{},"outputs":[],"source":["### Data type conversion:\n"]},{"cell_type":"markdown","id":"4a8e42fb-c167-4933-a081-34681fd9dbc7","metadata":{},"outputs":[],"source":["- **astype(str)**: \n","  - Used to convert the `'continent'` column to string type.\n","\n","- **fillna(0)**: \n","  - Replaces any NaN values with 0, ensuring that the numerical fields do not contain any missing data.\n","\n","- **astype('int64')**: \n","  - Converts the columns from potentially mixed types to 64-bit integers for consistent numerical representation.\n"]},{"cell_type":"markdown","id":"4d348633-eb97-4287-8f74-999969660654","metadata":{},"outputs":[],"source":["### Creating a Spark DataFrame:\n"]},{"cell_type":"markdown","id":"99815da4-edf6-4494-8d4e-290ad1419246","metadata":{},"outputs":[],"source":["- **createDataFrame**:\n","  - The `createDataFrame` method of the Spark session (`spark`) is called with `vaccination_data` (the Pandas DataFrame) as its argument.\n","  - **Parameters**:\n","    - It takes as input a subset of the pandas DataFrame that corresponds to the fields defined in the schema, accessed using `schema.fieldNames()`.\n","- This function automatically converts the Pandas DataFrame into a Spark DataFrame, which is designed to handle larger data sets across a distributed environment.\n"]},{"cell_type":"markdown","id":"a7999fb9-d772-439e-b3c4-bf3d1cc65553","metadata":{},"outputs":[],"source":["- The resulting spark_df will have the defined schema, which ensures consistency and compatibility with Spark's data processing capabilities.\n"]},{"cell_type":"markdown","id":"1411dfe1-382b-4edb-9d82-c8175e4c3df6","metadata":{},"outputs":[],"source":["### Storing the result:\n"]},{"cell_type":"code","id":"1cdde4c4-13b3-43b3-a351-2b082d1d9c0f","metadata":{},"outputs":[],"source":["# Convert to Spark DataFrame directly\n# Define the schema\nschema = StructType([\n    StructField(\"continent\", StringType(), True),\n    StructField(\"total_cases\", LongType(), True),\n    StructField(\"total_deaths\", LongType(), True),\n    StructField(\"total_vaccinations\", LongType(), True),\n    StructField(\"population\", LongType(), True)\n])\n\n# Convert the columns to the appropriate data types\nvaccination_data['continent'] = vaccination_data['continent'].astype(str)  # Ensures continent is a string\nvaccination_data['total_cases'] = vaccination_data['total_cases'].fillna(0).astype('int64')  # Fill NaNs and convert to int\nvaccination_data['total_deaths'] = vaccination_data['total_deaths'].fillna(0).astype('int64')  # Fill NaNs and convert to int\nvaccination_data['total_vaccinations'] = vaccination_data['total_vaccinations'].fillna(0).astype('int64')  # Fill NaNs and convert to int\nvaccination_data['population'] = vaccination_data['population'].fillna(0).astype('int64')  # Fill NaNs and convert to int\n\nspark_df = spark.createDataFrame(vaccination_data[schema.fieldNames()])  # Use only the specified fields\n# Show the Spark DataFrame\nspark_df.show()"]},{"cell_type":"markdown","id":"e8efcae4","metadata":{},"outputs":[],"source":["## 8. Checking the structure of the Spark DataFrame\n"]},{"cell_type":"markdown","id":"8e429aa1","metadata":{},"outputs":[],"source":["In this section, Let\\'s examine the structure of the Spark DataFrame that we created from the Pandas DataFrame. Understanding the schema of a DataFrame is crucial as it provides insight into the data types of each column and helps ensure that the data is organized correctly for analysis.\n"]},{"cell_type":"markdown","id":"49166db1-b330-4f52-b99e-910170287f3a","metadata":{},"outputs":[],"source":["### Displaying the schema:\n","\n","- The method `spark_df.printSchema()` is called to output the structure of the Spark DataFrame.\n","- This method prints the names of the columns along with their data types (e.g., `StringType`, `IntegerType`, `DoubleType`, etc.), providing a clear view of how the data is organized.\n"]},{"cell_type":"code","id":"555080e5","metadata":{},"outputs":[],"source":["print(\"Schema of the Spark DataFrame:\")\nspark_df.printSchema()\n# Print the structure of the DataFrame (columns and types)"]},{"cell_type":"markdown","id":"d1b3d525","metadata":{},"outputs":[],"source":["## 9. Basic data exploration\n"]},{"cell_type":"markdown","id":"5358f341","metadata":{},"outputs":[],"source":["In this section, let\\'s perform basic data exploration on the Spark DataFrame. This step is essential for understanding the data set better, allowing us to gain insights and identify any patterns or anomalies. Let\\'s demonstrate how to view specific contents of the DataFrame, select certain columns, and filter records based on conditions.\n"]},{"cell_type":"markdown","id":"93d9f700-26fb-423f-b434-b5a14b849c0a","metadata":{},"outputs":[],"source":["### 9.1 Viewing DataFrame contents\n"]},{"cell_type":"markdown","id":"f4df4892-8b53-4be1-95e6-7eb41a5e5ded","metadata":{},"outputs":[],"source":["- To view the contents in the DataFrame, use the following code:\n"]},{"cell_type":"code","id":"e31c016e-8623-47f7-8562-7c15254e1109","metadata":{},"outputs":[],"source":["# List the names of the columns you want to display\ncolumns_to_display = ['continent', 'total_cases', 'total_deaths', 'total_vaccinations', 'population']\n# Display the first 5 records of the specified columns\nspark_df.select(columns_to_display).show(5)"]},{"cell_type":"markdown","id":"31bb630c-52a0-4448-bb49-6d217195a169","metadata":{},"outputs":[],"source":["### 9.2 Picking specific columns\n"]},{"cell_type":"markdown","id":"3fb1c756-291d-40f0-994f-1b79307c5088","metadata":{},"outputs":[],"source":["- To display only certain columns, use the following code:\n"]},{"cell_type":"code","id":"a2cd6146-c8a8-4761-8b46-8d10e48bb29c","metadata":{},"outputs":[],"source":["print(\"Displaying the 'continent' and 'total_cases' columns:\")\n# Show only the 'continent' and 'total_cases' columns\nspark_df.select('continent', 'total_cases').show(5)"]},{"cell_type":"markdown","id":"a48d070b-2152-4381-8e46-4060f8c5c0d0","metadata":{},"outputs":[],"source":["### 9.3 Sifting Through Data\n"]},{"cell_type":"markdown","id":"85acf63c-4ef6-4b2b-bfa9-e6d479e215e5","metadata":{},"outputs":[],"source":["- To filter records based on a specific condition, use the following code:\n"]},{"cell_type":"code","id":"864c3317-aaf2-4690-8f95-aa5bf6568c14","metadata":{},"outputs":[],"source":["print(\"Filtering records where 'total_cases' is greater than 1,000,000:\")\n # Show records with more than 1 million total cases\nspark_df.filter(spark_df['total_cases'] > 1000000).show(5) "]},{"cell_type":"markdown","id":"35cb2722","metadata":{},"outputs":[],"source":["## 10. Working with columns\n"]},{"cell_type":"markdown","id":"d746713d","metadata":{},"outputs":[],"source":["In this section, let\\'s create a new column called `death_percentage`, which calculates the death rate during the COVID-19 pandemic. This calculation is based on the total_deaths (the count of deaths) and the population (the total population) columns in our Spark DataFrame. This new metric will provide valuable insight into the impact of COVID-19 in different regions.\n"]},{"cell_type":"markdown","id":"8a76e0c1-10f8-4486-8ec4-2d3e94e9ae67","metadata":{},"outputs":[],"source":["- Let\\'s import the functions module from `pyspark.sql` as `F`, which contains built-in functions for DataFrame operations.\n"]},{"cell_type":"markdown","id":"88b67498-7868-4ea0-abcf-503ee37e3fdc","metadata":{},"outputs":[],"source":["### Calculating the death percentage:\n"]},{"cell_type":"markdown","id":"f4781768-63d5-450c-ac4e-683a577e5426","metadata":{},"outputs":[],"source":["- Let\\'s create a new DataFrame `spark_df_with_percentage` by using the `withColumn()` method to add a new column called `death_percentage`.\n","- The formula `(spark_df['total_deaths'] / spark_df['population']) * 100` computes the death percentage by dividing the total deaths by the total population and multiplying by 100.\n"]},{"cell_type":"markdown","id":"c52934a8-ac14-414b-acf3-1a8eeb81e2c5","metadata":{},"outputs":[],"source":["### Formatting the percentage:\n"]},{"cell_type":"markdown","id":"37040078-0153-4610-be51-7cd0017f9ca3","metadata":{},"outputs":[],"source":["- Let\\'s update the `death_percentage` column to format its values to two decimal places using `F.format_number()`, and concatenate a percentage symbol using `F.concat()` and `F.lit('%')`.\n","- This makes the death percentage easier to read and interpret.\n"]},{"cell_type":"markdown","id":"d557b00b-0315-4511-becf-6762bb1a713b","metadata":{},"outputs":[],"source":["### Selecting relevant columns:\n"]},{"cell_type":"markdown","id":"13a38bfe-d663-48a8-b544-c0f9b9f8391c","metadata":{},"outputs":[],"source":["- Let\\'s define a list `columns_to_display` that includes `'total_deaths', 'population', 'death_percentage', 'continent', 'total_vaccinations', and 'total_cases'`.\n","- Finally, let's display the first five records of the modified DataFrame with the new column by calling `spark_df_with_percentage.select(columns_to_display).show(5)`.\n"]},{"cell_type":"code","id":"7c4cd634","metadata":{},"outputs":[],"source":["from pyspark.sql import functions as F\n\nspark_df_with_percentage = spark_df.withColumn(\n    'death_percentage', \n    (spark_df['total_deaths'] / spark_df['population']) * 100\n)\nspark_df_with_percentage = spark_df_with_percentage.withColumn(\n    'death_percentage',\n    F.concat(\n        # Format to 2 decimal places\n        F.format_number(spark_df_with_percentage['death_percentage'], 2), \n        # Append the percentage symbol \n        F.lit('%')  \n    )\n)\ncolumns_to_display = ['total_deaths', 'population', 'death_percentage', 'continent', 'total_vaccinations', 'total_cases']\nspark_df_with_percentage.select(columns_to_display).show(5)"]},{"cell_type":"markdown","id":"e317b1d6","metadata":{},"outputs":[],"source":["## 11. Grouping and summarizing\n"]},{"cell_type":"markdown","id":"5dba2c4d","metadata":{},"outputs":[],"source":[" Let\\'s calculate the total number of deaths per continent using the data in our Spark DataFrame. Grouping and summarizing data is a crucial aspect of data analysis, as it allows us to aggregate information and identify trends across different categories.\n"]},{"cell_type":"markdown","id":"0562debc-6a53-4b2e-b9b5-126a01a976d7","metadata":{},"outputs":[],"source":[" ### Grouping the data\n"]},{"cell_type":"markdown","id":"88621b79-42e1-48d6-8d16-6207cc418bdb","metadata":{},"outputs":[],"source":["The `spark_df.groupby(['continent'])` method groups the data by the `continent` column. This means that all records associated with each continent will be aggregated together.\n"]},{"cell_type":"markdown","id":"f8e588b9-8907-4b4e-9d72-5cee84bc1631","metadata":{},"outputs":[],"source":["### Aggregating the deaths\n"]},{"cell_type":"markdown","id":"2d82e1dc-169b-4e2c-ae90-f8faa52118de","metadata":{},"outputs":[],"source":["The `agg({\"total_deaths\": \"SUM\"})` function is used to specify the aggregation operation. In this case, we want to calculate the sum of the `total_deaths` for each continent. This operation will create a new DataFrame where each continent is listed alongside the total number of deaths attributed to it.\n"]},{"cell_type":"markdown","id":"b92335c9-47d2-4996-8cb5-602ebb50a8bb","metadata":{},"outputs":[],"source":["### Displaying the results\n"]},{"cell_type":"markdown","id":"726aec32-ef77-459b-ad91-96f1a02363bf","metadata":{},"outputs":[],"source":["The `show()` method is called to display the results of the aggregation. This will output the total number of deaths for each continent in a tabular format.\n"]},{"cell_type":"code","id":"ed954475","metadata":{},"outputs":[],"source":["print(\"Calculating the total deaths per continent:\")\n# Group by continent and sum total death rates\nspark_df.groupby(['continent']).agg({\"total_deaths\": \"SUM\"}).show()  "]},{"cell_type":"markdown","id":"8ee06742","metadata":{},"outputs":[],"source":["## 12. Exploring user-defined functions (UDFs)\n"]},{"cell_type":"markdown","id":"f32e2025","metadata":{},"outputs":[],"source":["\n","UDFs in PySpark allow us to create custom functions that can be applied to individual columns within a DataFrame. This feature provides increased flexibility and customization in data processing, enabling us to define specific transformations or calculations that are not available through built-in functions. In this section, let\\'s define a UDF to convert total deaths in the dataset.\n"]},{"cell_type":"markdown","id":"e22048a5-07fd-4ff8-bf67-9c28bd2fe44b","metadata":{},"outputs":[],"source":["### Importing pandas_udf\n"]},{"cell_type":"markdown","id":"ddac5f44-dcd5-40a1-832c-f16adcb4b7d9","metadata":{},"outputs":[],"source":["The `pandas_udf` function is imported from `pyspark.sql.functions`. This decorator allows us to define a UDF that operates on Pandas Series\n"]},{"cell_type":"markdown","id":"685a1b22-6c06-450d-a30f-7042a1ef3bc0","metadata":{},"outputs":[],"source":["### Defining the UDF\n"]},{"cell_type":"markdown","id":"37a7bfc2-cc4f-4f4c-85fc-66d4207f6165","metadata":{},"outputs":[],"source":["This function `convert_total_deaths()` takes in a parameter `total_deaths` and returns double its value. You can replace the logic with any transformation you want to apply to the column data.\n","\n"]},{"cell_type":"markdown","id":"4a42d792-fb46-4c1f-b54a-86da693ea0fd","metadata":{},"outputs":[],"source":["### Registering the UDF\n"]},{"cell_type":"markdown","id":"21035ca1-61a8-4d1e-bc2d-f9aa0ab2788f","metadata":{},"outputs":[],"source":["The line `spark.udf.register(\"convert_total_deaths\", convert_total_deaths, IntegerType())` registers the UDF with Spark indicating that the function returns an integer, allowing us to use it in Spark SQL queries and DataFrame operations.\n"]},{"cell_type":"code","id":"724fa2ec-6a45-4f26-9555-040e129b3dd0","metadata":{},"outputs":[],"source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.types import IntegerType\n# Function definition\ndef convert_total_deaths(total_deaths):\n    return total_deaths * 2\n# Here you can define any transformation you want\n# Register the UDF with Spark\nspark.udf.register(\"convert_total_deaths\", convert_total_deaths, IntegerType())"]},{"cell_type":"markdown","id":"40388f73","metadata":{},"outputs":[],"source":["## 13. Using Spark SQL\n"]},{"cell_type":"markdown","id":"0da3148b","metadata":{},"outputs":[],"source":["Spark SQL enables us to execute SQL queries directly on DataFrames.\n"]},{"cell_type":"code","id":"86424dab","metadata":{},"outputs":[],"source":["# Drop the existing temporary view if it exists\nspark.sql(\"DROP VIEW IF EXISTS data_v\")\n\n# Create a new temporary view\nspark_df.createTempView('data_v')\n\n# Execute the SQL query using the UDF\nspark.sql('SELECT continent, total_deaths, convert_total_deaths(total_deaths) as converted_total_deaths FROM data_v').show()"]},{"cell_type":"markdown","id":"0860017d","metadata":{},"outputs":[],"source":["## 14. Running SQL queries\n"]},{"cell_type":"markdown","id":"39d9a201","metadata":{},"outputs":[],"source":["In this step, let\\'s execute SQL queries to retrieve specific records from the temporary view which was created earlier. Let\\'s demonstrate how to display all records from the data table and filter those records based on vaccination totals. This capability allows for efficient data exploration and analysis using SQL syntax.\n"]},{"cell_type":"markdown","id":"78ff2ce1-2b36-4056-8d13-47b13ece5864","metadata":{},"outputs":[],"source":["## Displaying All Records\n"]},{"cell_type":"markdown","id":"34f37109-ab25-4fc8-b8c3-f4d57992ae7b","metadata":{},"outputs":[],"source":["The first query retrieves all records from the temporary view data using the SQL command SELECT * FROM data_v. The show() method is called to display the results in a tabular format. This is useful for getting an overview of the entire dataset.\n"]},{"cell_type":"code","id":"bfd78c13-a5c8-4d06-a0bb-bfb85225db3a","metadata":{},"outputs":[],"source":["spark.sql('SELECT * FROM data_v').show()"]},{"cell_type":"markdown","id":"804612f2-1d75-413d-b0fd-26ad05d1a9da","metadata":{},"outputs":[],"source":["### Filtering records\n"]},{"cell_type":"markdown","id":"edbd1fb9-e27f-4612-be6b-883791bb4187","metadata":{},"outputs":[],"source":["The second query is designed to filter the data set to show only those continents where the total vaccinations exceed 1 million. The SQL command used here is `SELECT continent FROM data_v WHERE total_vaccinations > 1000000`. The `show()` method is again used to display the results, specifically listing the continents that meet the filter criteria.\n"]},{"cell_type":"code","id":"a146173a-0b9f-477b-b81c-552309c52e73","metadata":{},"outputs":[],"source":["print(\"Displaying continent with total vaccinated more than 1 million:\")\n# SQL filtering\nspark.sql(\"SELECT continent FROM data_v WHERE total_vaccinations > 1000000\").show()"]},{"cell_type":"markdown","id":"bfb69c40","metadata":{},"outputs":[],"source":["## Conclusion\n"]},{"cell_type":"markdown","id":"0fed5595","metadata":{},"outputs":[],"source":["This hands-on-lab is designed to help you understand the core functionalities of robust tools for data manipulation and analysis. By mastering these tools, you gain the ability to choose the right one for your data processing needs, whether working with large data sets requiring distributed computing or smaller data sets that lend themselves to quick manipulation in memory.\n"]},{"cell_type":"markdown","id":"a8559536","metadata":{},"outputs":[],"source":["##  Author\n"]},{"cell_type":"markdown","id":"f8d3347d","metadata":{},"outputs":[],"source":["**Ritika Joshi**\n"]},{"cell_type":"markdown","id":"b2825e36-a979-4908-89c6-8730fd3c7e32","metadata":{},"outputs":[],"source":["<!--## Change Log\n","|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n","|-|-|-|-|\n","|2024-09-23|0.1|Ritika|First Draft|\n","--!>\n"]},{"cell_type":"code","id":"c7531e46-4d1d-4024-96a2-26169b541623","metadata":{},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"prev_pub_hash":"45a0dd703bfbcc37ede89eec12aa2a3ff03ef47840aa569cd67704c66ddb0f6c"},"nbformat":4,"nbformat_minor":5}